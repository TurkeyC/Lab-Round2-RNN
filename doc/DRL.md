# 深度强化学习

何毓辉

he.yuhui.ime@gmail.com

2022年11月1日

# 目录

1 引言 3  
2 基于模拟值的深度Q值网络 5

2.1 “吃还是不吃？这是个问题” 5  
2.2 Bellman方程 6

2.2.1 直观推导 7  
2.2.2 现实简化 9  
2.2.3 若干关键特征 10

2.3 Bellman方程求解 11

2.3.1 Q值表更新 11  
2.3.2 利用还是探索？  $\epsilon$  -贪婪算法 14  
2.3.3 在线还是离线策略：SARSA算法 15

2.4 从Q值表到深度Q值网络 16  
2.5 若干优化技术 19

2.5.1 目标Q值网络 19  
2.5.2 经验回放 21  
2.5.3 深度Q值网络训练：高级版 23  
2.5.4 训练结果讨论 23

2.6 忆阻突触阵列实现 24

# 3 基于脉冲的深度Q值网络 27

3.1 脉冲Q值学习 27  
3.2 策略迁移 29

# 4 本章小结 30

# 参考文献 32

# 1 引言

强化学习(reinforcement learning)，也叫做基于奖励的学习(reward-based learning)。如图1.1所示，在主体与环境的互动中，主体从环境中观测到信号  $O_{t}$ ，主体的神经网络据此计算，然后产生输出指令对环境执行一个动作  $A_{t}$ ，最后由环境对主体的输出结果进行判定  $R_{t}$ ，正确受到奖励(postive reward)，错误被惩罚(negative reward)，通过这种赏罚分明的奖惩机制“诱导”神经网络自我调整，让输出结果变得正确，从而获取更多的奖励。

![](images/7256640ee4e3d73dee69d7abf5d384540002e59e819894907c9ddc9f4b0aedba.jpg)  
图1.1：强化学习：主体与环境的互动。主体从环境中得到一个观察  $(O_{t})$ ，根据观察对环境采取行动  $(A_{t})$ ，从而获得来自环境的奖励  $(R_{t})$ ，注意当奖励为负值，意味着惩罚。

这种算法乍一听，就像小朋友在家长的督促下写作业，结果正确就赏一颗糖，错误则挨一顿揍（划掉）。看到这里，读者很自然地就会想到一个问题：那强化学习跟监督学习的区别在哪里呢？

图1.2老鼠走迷宫这个经典案例可以用来较好地回答这个问题。从老

鼠所在位置到出口，实际上存在多条路线。对于监督学习算法，是预先指定了其中一条，在老鼠探索的过程中，任何偏离该指定路线的行为都会被“纠正”。而对于强化学习算法，则是只要老鼠最后能走出迷宫即可，路线任其探索。

![](images/a4df86471df3425cce318b364fdd8bed29a6ccd9f84641fd0df1b7eab683c018.jpg)  
图1.2：老鼠走迷宫：监督与强化学习对比。假设从迷宫起点到出口存在多条可以走出的路线，如图中不同颜色虚线所示。监督学习的做法是主体必须走规定的某一条路线，以该路线上的每一个步骤为标签来训练神经网络，不允许有任何偏离，甚至改进。强化学习则是只要能够走出来，并且代价越小越好，鼓励探索和改进。

回到小朋友在家长督促下写作业的问题，可以看到，监督学习对应的是一个专制的家长，他不仅要小朋友做出正确答案，还要小朋友必须按照他规定的解法做出来；而强化学习对应的是一个开明的家长，只要小朋友最后做出正确答案即可，用哪种解法不做限制，自己探索。

生活常识告诉我们，后一种学习方式更鼓励创造性。这也是为什么像AlphaGo这样的实际应用采用的是强化学习算法。众所周知，围棋可能的下法组合几乎是天文数字，在这种情况下，监督学习所需要的对每一种下法贴标签(labeled data)事实上是做不到的。

另一方面，上述讨论让我们注意到，监督学习与强化学习算法层面既有区别也有相似之处。这意味着，我们在接下来对强化学习算法和硬件实现的讨论中，应该充分利用它们在形式上的某些相似性，从而复用已经发展得比较成熟的监督学习型神经网络的软硬件成果。

# 强化学习的关键特征：

- 没有监督信号，只有正/负回馈(positive/negative reward)信号；  
- 当前行动导致的奖惩反馈信号不是即时(instantaneous)的，而是延迟(delayed)给出的；  
- 输入数据是时变的(time-dependent)，并且，当前步骤采取的行动不同，下个步骤对应的输入会不一样，即输入数据不是独立同分布(independent identically distribution，IID)的。

# 2 基于模拟值的深度Q值网络

# 2.1 “吃还是不吃？这是个问题”

我们以一个简单的游戏来理解强化学习是怎么定义和处理问题的。

![](images/67b1f98bc4e94e44e1dfdabe7ba1337f4ef9a597941a98629fd559e5bfe49057.jpg)  
图2.1: “吃还是不吃”: 强化学习示例。假设主体存在“饱”、“饿”、“饿死”三种状态, 以及 “吃” 和 “不吃” 两种动作选择。图显示的是处在不同的状态, 采取不同的动作, 在状态之间转换的概率  $p$  以及当下收益  $r$  。

如图2.1所示，假设智能体处在“饱”、“饿”、“饿死”三个状态之一，从“吃”或者“不吃”两种行动中选择一个。以处在“饿”这个状态为例，

假如选择“吃”这个行动，有  $90\%$  的概率变“饱”了，意味着获得了  $+1$  的收益，但还有  $10\%$  的概率仍然饿着，于是获得  $-1$  的收益；假如选择“不吃”，有  $90\%$  的概率仍然“饿”着，对应地获得  $-1$  的收益，还有  $10\%$  的概率直接“饿死”了，对应  $-10$  的收益。

以此类推，我们稍加计算就可以列出下面这张表，统计各种状态下采取各种行动的直接收益。

表 2.1: 吃-不吃的直接收益表  

<table><tr><td>直接收益状态\动作</td><td>吃</td><td>不吃</td></tr><tr><td>饿</td><td>0.8</td><td>-1.9</td></tr><tr><td>饱</td><td>0</td><td>0</td></tr><tr><td>饿死</td><td>-</td><td>-</td></tr></table>

注意上述示例做了高度简化，例如不存在“饱”的状态下继续“吃”于是“撑”了的选项。而“饿死”则对应着游戏的终结。

现在我们要解决的问题是，当智能体处在“饿”这个状态时，应该选择哪种行动策略收益最大？

$$
\max  _ {\text {行 动}} \{\text {预 期 收 益} (\text {状 态 ， 行 动}) \} \tag {2.1}
$$

即“预期收益”是当前“状态”和“行动”的函数，要根据一个预定的“策略”选择一种“行动”，使“预期收益”最大化。

# 2.2 Bellman方程

从上一节的例子可以看到，当我们试图用强化学习来处理问题时，通常需要定义：

1 智能体可能处在的状态集合  $\{s\}$  : 饱、饿、饿死  
2 智能体可能采取的动作集合  $\{a\}$  ：吃、不吃

3. 状态之间通过动作实现转换的概率表  $\{P_{ss'}^a\}$  ：饿  $\rightarrow$  （可能）饱了、饿  $\rightarrow$  （可能）还饿、饿  $\rightarrow$  （可能）继续饿、饿  $\rightarrow$  （可能）饿死……

4 每一次动作导致状态切换而获得的当前直接收益表  $\{R_{ss'}^a\}$  ：饿  $\rightarrow$  饱了  $\Rightarrow$  开心  $(+1)$  、饿  $\rightarrow$  还饿  $\Rightarrow$  不开心  $(-1)$  、饿  $\rightarrow$  饿死  $\Rightarrow$  悲剧  $(-10)$

5 状态-行动的价值函数  $Q(s, a)$  ：在当前饱/饿状态下，采取行动吃/不吃，对应的总收益；

6 行动策略  $\pi(s, a)$ ：在当前饱/饿状态下，采取行动吃/不吃的概率；

7 状态的价值函数  $Q(s)$  ：既然在状态  $s$  下采取不同行动  $a / a'$  对应的价值  $Q(s, a / a')$  不同，那么状态本身的价值  $Q(s)$  就是该状态下所有行动对应的价值  $Q(s, a)$  做加权叠加  $Q(s) = \sum_{a} w(a) Q(s, a)$ ，其中权重因子  $w(a)$  代表采取行动  $a$  的概率。很显然，采取高价值行动的可能性（权重因子）越大，该状态的价值就越高。

从前述例子可以看到，所谓强化学习，就是寻找最优的行动策略  $\pi^{*}(s,a)$ ，使得对应状态  $s$  的总体收益  $Q(s)$  最大：

$$
\pi^ {*} (s, a) \Rightarrow \max  Q (s) \tag {2.2}
$$

从前述例子可以看到，总收益应该是每一步行动收益的叠加：

$$
Q (s, a) = R _ {s s ^ {\prime}} ^ {a} + R _ {s ^ {\prime} s ^ {\prime \prime}} ^ {a ^ {\prime}} + R _ {s ^ {\prime \prime} s ^ {\prime \prime \prime}} ^ {a ^ {\prime \prime}} + \dots \tag {2.3}
$$

很显然，我们需要首先推导出状态-行动价值函数  $Q(s, a)$  的表达式，即下文要讨论的Bellman方程。

# 2.2.1 直观推导

Bellman方程的物理含义是非常清晰的，事实上只要清晰理解了它的逻辑，不需要复杂的推导就可以直接写出来：

首先，假设智能体在当前状态  $s$  采取行动  $a$  ，会跳转到状态  $s^{\prime}$  ，即 $s\stackrel {a}{\to}s^{\prime}$  ，那么描述未来总收益的状态-行动价值函数  $Q(s,a)$  就可以写作：

$$
Q (s, a) = R _ {s s ^ {\prime}} ^ {a} + \gamma Q (s ^ {\prime})
$$

注意上述表达式是一个递归表达式，首先是当前状态下这个动作本身的收益  $R_{ss'}^a$ ，以及通过这个动作进入下一个状态  $s'$  后的该状态价值函数  $Q(s')$ 。

另外，上述表达式对未来收益引入了一个折扣因子  $\gamma$ ：对于遥远未来的收益，在当下考虑每一种选择的总收益时需要打折计入。

思考题: 折扣因子是生活中 “远水不解近渴” 的意思吗?

另一方面，从价值函数  $Q(s')$  的定义我们可以知道，一个状态的价值  $Q(s')$  其实是当前状态下，可能采取的各种不同行动  $\{a'\}$  所得的行动-价值函数  $\{Q(s', a')\}$  做一个加权平均，权重因子就是采取不同行动  $\{a'\}$  的概率  $\{\pi(s', a')\}$ ，如图2.2：

![](images/f957cd5d9129c1ff1707ae67368827b8a17d48c728ac6764e78877b7e54a4323.jpg)  
图2.2：状态  $s$  的价值示意图。处在  $s$  状态，分别以  $\pi (s,a^{\prime})$  、  $\pi (s^{\prime},a^{\prime \prime})$  、 $\pi (s^{\prime},a^{\prime \prime \prime})$  的概率执行动作  $a^\prime$  、  $a''$  、  $a'''$  ，达到  $s^{\prime}$  、  $s''$  、  $s'''$  新状态，同时或得这一步的收获  $R_{ss'}^{a'}$  、  $R_{ss''}^{a''}$  、  $R_{ss''}^{a''\prime \prime}$  。

$$
Q ^ {\pi} (s ^ {\prime}) = \sum_ {a ^ {\prime}} \pi (s ^ {\prime}, a ^ {\prime}) Q ^ {\pi} (s ^ {\prime}, a ^ {\prime})
$$

其中  $\pi(s', a')$  表示在  $s'$  状态下采取  $a'$  行动的概率，满足总概率归一化条件： $\sum_{a'} \pi(s', a') = 1$ ，于是我们得到表达式：

$$
Q ^ {\pi} (s, a) = R _ {s s ^ {\prime}} ^ {a} + \gamma \sum_ {a ^ {\prime}} \pi (s ^ {\prime}, a ^ {\prime}) Q ^ {\pi} (s ^ {\prime}, a ^ {\prime})
$$

进一步，从前述“饿/饱/吃/不吃”的简单示例中，我们不难发现，在当前状态  $s$  下，采取行动  $a$  ，到达的状态  $s^{\prime}$  可能不止一种，如图2.3所示。用数学语言来描述就是需要定义  $P_{ss^{\prime}}^{a}$  描述  $s \xrightarrow{a} s^{\prime}$  的概率，且满足总概率归一化条件： $\sum_{s^{\prime}} P_{ss^{\prime}}^{a} = 1$  。相应地，考虑到不同的末态可能，上述表达式需要改为加权平均，于是我们得到Bellman方程完整表达式：

![](images/26ab2fcdb064cb3a3dac41d6ea78bf62890f512a0fa7f69abd2e3a3d052fd373.jpg)  
图2.3：状态-价值函数  $Q(s,a)$  的示意图。

$$
Q ^ {\pi} (s, a) = \sum_ {s ^ {\prime}} P _ {s s ^ {\prime}} ^ {a} \left[ R _ {s s ^ {\prime}} ^ {a} + \sum_ {a ^ {\prime}} \pi \left(s ^ {\prime}, a ^ {\prime}\right) \gamma Q ^ {\pi} \left(s ^ {\prime}, a ^ {\prime}\right) \right] \tag {2.4}
$$

# 2.2.2 现实简化

在一个经典(classical)且决定论(deterministic)的世界里，Bellman方程可以进一步到简化。

# 经典世界

在一个宏观经典世界里，一个智能体在同一个时刻只能做出一个行动，而不可能以不同的概率同时做出多个行动。换句话说，方程2.4里的  $\pi(s, a)$  是统计学意义上的，控制其它条件不变，多次重复该  $s$  状态下统计出来采取不同行动  $a$ 、 $a'$ 、 $a''$  等的概率。形象地说，在真实的宏观世界里，对于某一次“饿/饱/吃/不吃”实验，只能决定采取“吃”或“不吃”。也就是说，对于  $\pi(s, a)$ ，只有某一个  $a$  对应的行动被采取了， $\pi(s, a) = 1$ ，而其它所有的  $\bar{a}$  行动没有实行， $\pi(s, \bar{a}) = 0$ 。

$$
\pi (s, a) = \left\{ \begin{array}{l} 1 \\ 0 \end{array} \right. \tag {2.5}
$$

# - 决定论世界

我们知道，在18世纪伴随着近代物理学的爆发，出现一种机械唯物论，认为给定了系统当前的状态和外加的力，系统的未来演化就是完全确定了，或者说，只要给定的状态和力的精度足够，理论上是可以根据牛顿力学定理准确计算出状态的未来演化。

对该机械唯物论谬误的讨论超出了本书范围，但我们假设在一些严格限定的简单日常生活情况下，上述决定论是适用的。这意味着Bellman方程中的状态转换概率  $P_{ss'}^{a}$  只能是非零即一的。

$$
P _ {s s ^ {\prime}} ^ {a} = \left\{ \begin{array}{l} 1 \\ 0 \end{array} \right. \tag {2.6}
$$

思考题：乍一看，“饿/饱/吃/不吃”实验似乎是一个反例：在当前“饿”的情况下，吃了一个馒头，可能饱了，也可能仍然饿着呀？这种情况下如何理解决定论世界？

考虑到上述两个条件，Bellman方程描述的现实最佳策略  $\pi^{*}(s,a)$  就是下述方程：

$$
Q (s, a) = R _ {s s ^ {\prime}} ^ {a} + \gamma \max  _ {a ^ {\prime}} Q \left(s ^ {\prime}, a ^ {\prime}\right) \tag {2.7}
$$

# 2.2.3 若干关键特征

从上述讨论可以看到，Bellman方程所描述的系统演化在形式上高度类似数字逻辑里的有限状态机：系统处在有限状态中的某一个，通过不同的行动（状态机里的输入）跳转到不同的状态，同时获得这一步跳转的回馈（状态机里的输出）。

我们列表总结如下：

# Bellman方程的若干关键特征：

1 需要定义5个量  $\{s, a, P_{ss'}, R_{ss'}, \gamma\}$ :

1.1 有限数目的状态  $\{s\}$ ，智能体在任意时刻必定处在其中一个；  
1.2 有限数目的行动  $\{a\}$ , 智能体在任意状态下总是从中选取一个;  
1.3 行动  $a$  导致从状态  $s$  到状态  $s^{\prime}$  的跳转概率  $\{P_{ss^{\prime}}^{a}\}$ ;  
1.4  $s \xrightarrow{a} s'$  这一步的即时收益  $\{R_{ss'}^a\}$ ;  
1.5 折扣因子  $\gamma$  。

2 Bellman方程适用于马可夫过程(Markov process)，即当前状态仅取决于前一个状态：

$$
P \left(s _ {t + 1} \mid s _ {t}\right) = P \left(s _ {t + 1} \mid s _ {1}, \dots , s _ {t}\right)
$$

3 Bellman方程是个递归(recursive)表达式，意味着可以通过迭代 (iteration)来求解。

# 2.3 Bellman方程求解

# 2.3.1 Q值表更新

如表2.2所示，状态-行动的价值函数  $Q(s, a)$  是一个二维矩阵，俗称Q值表(Q-table)。强化学习要做的，就是根据指定的行动策略，不停更新Q值表，直到下一轮更新引起的Q值表变化可以忽略，即Q值表收敛了，就认为学习完成了。

注意在公式2.7描述的简化情况下，我们会有如下两张表来描述系统状态，一张是状态转换表  $P_{ss'}^a$  （表2.3，注意其元素是非零即一的），另一张是状态转换直接收益表  $R_{ss'}^a$  （表2.4）。

那么这两张表从哪里来的呢？以前述老鼠走迷宫为例，在初始情况下，老鼠没有探索过迷宫，因此这两张表里的大部分元素对它而言是未知的。

表 2.2: Q值表  

<table><tr><td>Q值\动作
状态</td><td>a1</td><td>a2</td><td>aj</td><td>an</td></tr><tr><td>s1</td><td></td><td></td><td></td><td></td></tr><tr><td>s2</td><td></td><td></td><td></td><td></td></tr><tr><td>si</td><td></td><td></td><td>Q(i,j) = Q(si,aj)</td><td></td></tr><tr><td>sm</td><td></td><td></td><td></td><td></td></tr></table>

表 2.3: 状态转换表  

<table><tr><td>下个状态\动作
当前状态</td><td>a1</td><td>a2</td><td>aj</td><td>an</td></tr><tr><td>s1</td><td></td><td></td><td></td><td></td></tr><tr><td>s2</td><td></td><td></td><td></td><td></td></tr><tr><td>si</td><td></td><td></td><td>P(i,j) = si a_j si&#x27;</td><td></td></tr><tr><td>sm</td><td></td><td></td><td></td><td></td></tr></table>

表 2.4: 状态转换直接收益表  

<table><tr><td>直接收益\动作当前状态</td><td>a1</td><td>a2</td><td>aj</td><td>an</td></tr><tr><td>s1</td><td></td><td></td><td></td><td></td></tr><tr><td>s2</td><td></td><td></td><td></td><td></td></tr><tr><td>si</td><td></td><td></td><td>R(i,j) = RajSi,si&#x27;</td><td></td></tr><tr><td>sm</td><td></td><td></td><td></td><td></td></tr></table>

当它在各个状态  $\{s\}$  下不停地采取各种行动  $\{a\}$  时，环境会反馈给它对应的状态转换  $P_{s,s^{\prime}}^{a}$  以及直接收益  $R_{s,s^{\prime}}^{a}$  。换言之，当我们采取某种“上帝视角”看图1.1，把“环境”看作一种具有虚拟人格的存在，那么，每当“智能体”对“环境”采取了一个动作，“环境”就赶紧查阅它手里的两张表2.3和2.4，给予“智能体”反馈：一个是“智能体”本次动作的“收益”，另一个是“智能体”采取“行动”后转移到的下一个“状态”。

```txt
Initialize  $Q(s,a)$  arbitrarily   
Repeat (for each episode): Initialize s Repeat (for each step of episode): Choose  $\alpha$  from s using policy derived from Q e.g. maxQ(s,a) Take action a, observe r, s'  $Q(s,a)\gets Q(s,a) + \alpha \left[r + \gamma_{\max}Q(s',a') - Q(s,a)\right]$ $s\gets s^{\prime}$    
Until s is terminal
```

图2.4：Q值表更新的伪代码。

图2.4给出了Q值表更新的伪代码，操作流程如下：

# Q值表更新流程(基础版)

1. 列出  $m \times n$  维的 Q 值表, 并初始化为全零矩阵。其中  $m$  是状态数目,  $n$  是动作数目。  
2 假设系统给出若干个可能的初始态，选择其中一种状态  $s_{i}$  作为初始态，根据预定的行动策略开始动作，一步步转换状态，同时更新相应的 Q值。

以基于公式2.7的操作为例：

2.1 在状态  $s_{i}$  下拟采取的行动, 就是挑选 Q 值表 2.2 里  $s_{i}$  这一行最大值  $Q(i, j) = \max Q(i,:)$  对应的那个行动  $a_{j}$ ;  
2.2 然后查找两张表：一张是状态转换表2.3的元素  $P(i, j)$ ，从中找到下一个状态是  $s_{i^{\prime}}$ ；另一张是状态转换收益表2.4的元

素  $R(i,j)$  ，得知这一步行动的直接收益  $r$

2.3 查找Q值表中新状态  $s_{i'}$  所在的行，找到这一行最大值  $Q(i', j') = \max Q(i', :)$ ，然后根据下述公式更新Q值表：

$$
Q (i, j) \leftarrow Q (i, j) + \alpha \left[ r + \gamma \max  Q \left(i ^ {\prime}, :\right) - Q (i, j) \right] \tag {2.8}
$$

2.4 进入下一个状态  $s_{i^{\prime}}$  ，重复上述步骤，继续更新Q值表，直到系统设定的末态或者其它终止条件。

3 遍历系统其它可能的初始态，重复上述Q值学习、更新；

思考题: 为什么 Q 值表更新不直接采用  $Q(i, j) \gets r + \gamma \max Q(i', :)$ , 而是采用方程 2.8 的形式?

# 2.3.2 利用还是探索？  $\epsilon$  -贪婪算法

由方程2.7描述的，在Q值表的每一行找最大值对应的行动这一策略，实际上是非常容易掉进“局域最优”而非“全局最优”的陷阱。

考虑这样一个情况：当事人处在高中毕业这个状态，需要决定是读大学，还是去工作。这里假设Q值表仅评估经济收益，不包含比如更高等的教育带给人的精神提升等不便量化的收益。

很显然，读大学的话，接下来四年的个人财务收益通常为很大的负值，而直接工作带来的收益为正。然而，稍微看远一步就知道，正常情况下受过更高等的教育后，能够承担更专业、门槛更高的工作，因此四年后再工作的收入应该很快能够补回先前的经济损失。换句话说，这个案例的全局最优决策通常是选择读大学。然而，直接按照公式2.7决策，就会掉进“高中毕业后工作”这个眼前利益的陷阱。

造成这一困境的原因显然是智能体没有把眼光放得更长远，只考虑了“看得见”的当前最大收益。因此，一种对应的纠错策略—— $\epsilon-$ 贪婪  $(\epsilon-$  greedy)算法——被发展出来：引入随机性，跳过局域最优的陷阱。

具体来说，智能体在每一步行动决策时，掷一下骰子，产生一个[0,1]之间的随机数  $R$ ，假如  $R > \epsilon$ ，就按照方程2.7行动；假如  $R < \epsilon$ ，则随机地选

择一个行动。在Q值表刚开始更新阶段，设置较大的  $\epsilon$  值，使得很大比例的行动是随机的，目的是绕开眼前利益的陷阱，尝试其它路径以找到潜在的更大收益；随着Q值表更新的轮数越来越多，更多的路径探索结果已经反映在相关的Q值元素里，于是逐步减小  $\epsilon$  （见图2.5），使得越来越多的行动是以获取目前已知的最大Q值为目的的。可以看到，从效果上看，前一种做法是探索(exploration)环境，后一种是直接利用(exploitation)环境获得收益。

![](images/19b58f63c78e70bdfe6165c29bd5392a55fa9a0992e3fc8318f4e2242cb5244c.jpg)  
图2.5： $\epsilon$  演化：从探索到利用。

# 2.3.3 在线还是离线策略：SARSA算法

引入“探索或者利用”的随机性以后，假如仍然采取方程2.7形式，就可能在执行层面出现“脑臀分离”现象：在  $R < \epsilon$  的探索决策下，明明屁股的实际行动是随机选取了一个动作  $a_{R}$ ，然而脑子里的Q值更新仍然是取的  $\max_{a} Q(s, a)$ 。为了解决这个明显的不一致，SARSA(state-action-reward-state-action)算法被提出来：Q值更新按照实际采取的行动来计算，而不是不加区分地使用公式2.7。

# Q值表更新流程(SARSA版)

1. 列出  $m \times n$  维的 Q 值表，并初始化为全零矩阵。其中  $m$  是状态数目， $n$  是动作数目。  
2 假设系统给出若干个可能的初始态，选择其中一种状态  $s_{i}$  作为初始态，根据预定的行动策略开始动作，一步步转换状态，同时更新相应的 Q值。

基于SARSA算法的操作：

2.1 在状态  $s_i$  下，首先产生一个[0,1]之间的随机数  $R$ ，假如  $R > \epsilon$ ，拟采取的行动是挑选Q值表2.2里  $s_i$  这一行最大值  $Q(i,j) = \max Q(i,:)$  对应的那个行动  $a_j$ ；假如  $R < \epsilon$ ，拟采取的行动是在Q值表2.2里  $s_i$  这一行随机挑选一个行动  $a_r$ 。  
2.2 然后查找两张表：一张是状态转换表2.3的元素  $P(i, j / r)$ ，从中找到下一个状态是  $s_{i'}$ ；另一张是状态转换收益表2.4的元素  $R(i, j / r)$ ，得知这一步行动的直接收益  $r$ ；  
2.3 查找Q值表中新状态  $s_{i^{\prime}}$  所在的行，采取跟  $s_i$  状态处理的类似办法，确定对应的行动  $a_{j^{\prime}}$  ，然后根据下述公式更新Q值表：

$$
Q (i, j) \leftarrow Q (i, j) + \alpha \left[ r + \gamma Q \left(i ^ {\prime}, j ^ {\prime}\right) - Q (i, j) \right] \tag {2.9}
$$

2.4 进入下一个状态  $s_{i^{\prime}}$ ，重复上述步骤，继续更新Q值表，直到系统设定的末态或者其它终止条件。

3 遍历系统其它可能的初始态，重复上述Q值学习、更新；并且逐步减小  $\epsilon$  的设定值。

# 2.4 从Q值表到深度Q值网络

在实际应用中，上述Q值表更新法并不经济，有些情况下甚至不可行。以下围棋为例，我们知道，围棋可能的状态数几乎是天文数字，对应的Q值表是一个远超出现实计算机内存容量的矩阵，这种情况下Q值表的访问和更新效率会变得异常低下。

那么，研究人员是怎么想到用神经网络的形式来更新Q值表的呢？具体来说，Q值表更新公式2.8或者它的优化版本2.9求解是如何跟神经网络联系上的呢？

这里的关键点是，在数学形式上Bellman方程是递归(recursive)表达式，意味着它可以通过多轮迭代(iteration)来求解。那么在求解中，可以定义迭代前后的误差，既然能定义每一轮的误差，那么我们马上就能联想到在前述章节“人工神经网络的监督学习”里定义的误差极小化办法：梯度下降。

以强化学习的典型案例“倒立摆保持平衡”(cart-pole)为例，图2.6是它对应的深度神经网络结构示意图。采用小车位置、小车速度、悬杆角度、悬杆顶端线速度四个量作为神经网络的输入  $\vec{s}$ ，输出层使用两个神经元分别代表向左、向右的作用力。在训练中，使输出神经元编码相应的状态-价值函数  $Q(\vec{s},\leftarrow)$  、  $Q(\vec{s},\rightarrow)$  。

![](images/16be15c12b38e17d160a2ef040f5ed7130680ed27d1d263470a2996ac7faea76.jpg)  
图2.6：倒立摆问题的深度Q值网络。

定义损失函数(loss function):

$$
L (w) = \mathbb {E} \left[ \left(r + \gamma \max  _ {a ^ {\prime}} Q \left(s ^ {\prime}, a ^ {\prime}, w\right) - Q (s, a, w)\right) ^ {2} \right] \tag {2.10}
$$

方程2.10标蓝色的部分是状态-动作价值函数  $Q(s, a)$  的期望值(target)，可以看到，该损失函数的定义在数学形式上跟“人工神经网络的监督学习”里误差函数定义是一致的*。

于是，我们可以套用前述章节讲述的梯度下降法来更新突触权重，达到误差极小化的目的：

$$
\Delta w = - \alpha \left[ r + \gamma \max  _ {a ^ {\prime}} Q \left(s ^ {\prime}, a ^ {\prime}, w\right) - Q (s, a, w) \right] \nabla_ {w} Q (s, a, w) \tag {2.11}
$$

接下来，深度Q值网络的训练操作流程如图2.7所示。

![](images/2b47541b6540bcf0e8a445caa679025b4613a5f9708c0f917b9a6e4cda2fb59a.jpg)  
图2.7：深度Q值网络的训练流程图（基础版）。

![](images/11e72faaa6fab0f3684b26de750413b7bedb6d9993714a00ee2e4dc2ae177d8d.jpg)

# 深度Q值网络训练（基础版）：

1 建立起神经网络，首先初始化权重；  
2 对当前状态  $s$  选择行动  $a$  ：将状态  $s$  编码输入给神经网络，执行前向(forward)操作，得到输出，从中选择最大输出值  $\max_{a} Q(s, a)$  以及该输出神经元对应的行动  $a$ ;  
3 当环境接收到神经网络输出的行动  $a$  后，结合神经网络的输入状态  $s$ ，查找状态转换表2.3和状态转换直接收益表2.4，找到状态  $s$  下行动  $a$  的直接收益  $r$ ，以及下一个状态  $s'$ ；  
4 将状态  $s^{\prime}$  编码输入给神经网络，再次执行前向(forward)操作，得到输出，选择输出值最大的，即  $\max_{a^{\prime}} Q(s^{\prime}, a^{\prime})$ ;

5 计算目标Q值：  $Q_{\text {target}}(s, a) = r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime})$ ;  
6 根据公式2.11计算神经网络权重的变化，然后更新update)它们。

可以看到，深度Q值网络一次训练需要两次前向(forward)和一次更新update)操作：

# 前向  $\rightarrow$  前向  $\rightarrow$  更新

值得指出的是，从Q值表到深度Q值网络是一次状态泛化能力的飞跃。如表2.2所示，Q值表能处理的状态数是有限的，对应在状态空间里是有限个离散的点，对于状态空间那些没有训练到的点，Q值表无法给出行动策略，这意味着Q值表是不具备泛化能力的。

而深度Q值网络采用若干个输入神经元来编码状态，由于神经元可以连续取值，原则上深度Q值网络能够处理状态空间连续的点，也就是说深度Q值网络具有状态空间的泛化能力。

# 2.5 若干优化技术

# 2.5.1 目标Q值网络

公式2.10显示，深度Q值网络的目标Q值  $Q_{\text {target}}$  本身是在不停变化的，只要神经网络权重更新了，那么计算出来的  $Q_{\text {target}}$  就不一样了，相当于网络学习的目标本身在不断变化，这对网络训练的快速收敛是十分不利的。如图2.8所示，假如不做特殊处理，深度Q值网络就像一只在追咬自己尾巴的狗。

为了解决这个问题，人们发展出“目标Q值网络”技术，如图2.9所示，建立两张Q值网络，其中一个是“目标Q值网络”，另一个是“预期Q值网络”，分别对应公式2.10里的两项。在实际操作中，每次输入都同时赋值给两张网络，分别做forward计算，然后将各自结果放在一起相减来计算损失函数，据此得出Q值网络的权重更新情况。要注意，目标Q值网络和预期Q值网络的关键区别就在这里：后者每一次操作都会被更新连接权重，而前者则是保持相对不变，每  $N$  次才被更新一次权重。

通过这种办法，目标Q值在保持相对稳定和动态更新之间，取得了一定的平衡。

![](images/78002b8775e7889bcde31b04e93c1f26c6d680d572a05520a342c35d4d219233.jpg)  
图2.8：为什么要引入目标Q值网络。

![](images/e287849e956f690609bcdca103cb7ad260f691a6b77d000a0e8b9256961e52a8.jpg)  
图2.9：如何使用目标Q值网络。

# 2.5.2 经验回放

从深度Q值网络的定义可以看到，它跟“人工神经网络的监督学习”存在若干重大区别，不仅是上一节指出的前者目标值本身在不停演化，还有一个关键区别是，深度Q值网络的输入不再是独立同分布的。相反，它的输入流在时间上是有直接的逻辑关联：

$$
s \xrightarrow {a} s ^ {\prime} \xrightarrow {a ^ {\prime}} s ^ {\prime \prime} \xrightarrow {a ^ {\prime \prime}} s ^ {\prime \prime \prime} \xrightarrow {\dots} \dots \tag {2.12}
$$

在“人工神经网络的监督学习”中，对于实际应用中很大规模的测试集（比如人脸识别），通常是随机选取它的一个真子集，并且在训练中将该真子集的数据随机排序输入给神经网络，然后通过梯度下降来最小化误差，即随机梯度下降。而深度Q值网络的输入数据集通常也是状态全集的一个很小真子集，以图2.6展示的倒立摆问题为例，对于任意一个初始状态输入{小车位置，小车速度，悬杆偏移角度，悬杆顶端线速度}，随着时间演化，它既不可能也无必要遍历所有状态，即状态四个分量的全部可能组合。换句话说，对于深度Q值网络，实际训练也是选取了所有输入状态的一个很小真子集。

而从前述章节的讨论我们知道，在监督学习算法里，将训练数据随机排序输入给神经网络，有利于避开局域极小值的“陷阱”。

因此，既然深度Q值网络无论是算法（梯度下降以最小化损失函数），还是输入处理（取全部可能状态的一个真子集），都是在形式上高度雷同人工神经网络监督学习的随机梯度下降算法，那么，对深度Q值网络的输入流做随机处理，使之更加吻合后者，从而直接利用发展得很成熟的后者处理流程，是比较合理的。

这是引入如图2.10所示“经验回放”(experience replay)技术的一个考虑。另一个考虑如“经验回放”这个名称所示，鉴于每一轮训练都在修改Q值表，而Q值表任一元素的更新，都有可能引发后续训练中状态的不同走向，进而产生链式反应。因此，经常性地把前期训练结果重新代入，有可能避免当下某一步更新引起的“暴走”。

经验回放的操作流程如图2.10所示。

![](images/f41c17f4ccb5630db15eceb7974f1ad33a9ccfb7b279ac6ff46fc8c1a2f6276e.jpg)  
图2.10：经验回放。

# 2.5.3 深度Q值网络训练：高级版

考虑到“目标Q值网络”和“经验回放”技术后的深度Q值网络训练如图2.11所示。

![](images/6e0cdc6fa0701e1062577ad3e124078eafdde63b499829ee22bb14de111fa4f7.jpg)  
图2.11：深度Q值网络训练的伪代码（含目标Q值、经验回放技术）。

# 2.5.4 训练结果讨论

图2.12是采取上述“经验回放”、“目标Q值网络”技术的对照实验结果[1]，分别展示了“既没有经验回放，也没有目标Q值网络”、“只用到目标Q值网络”、“只用到经验回放”、“既用到经验回放，也用到目标Q值网络”四种设置下的深度Q值网络收益。

可以看到，对于多数游戏测试，“经验回放”技术对总收益的提升比“目标Q值网络”更显著，差距在数倍到几十倍，而两种技术的综合又比单独采取“经验回放”的效果要提升数倍。

<table><tr><td>Game</td><td>With replay, with target Q</td><td>With replay, without target Q</td><td>Without replay, with target Q</td><td>Without replay, without target Q</td></tr><tr><td>Breakout</td><td>316.8</td><td>240.7</td><td>10.2</td><td>3.2</td></tr><tr><td>Enduro</td><td>1006.3</td><td>831.4</td><td>141.9</td><td>29.1</td></tr><tr><td>River Raid</td><td>7446.6</td><td>4102.8</td><td>2867.7</td><td>1453.0</td></tr><tr><td>Seaquest</td><td>2894.4</td><td>822.6</td><td>1003.0</td><td>275.8</td></tr><tr><td>Space Invaders</td><td>1088.9</td><td>826.3</td><td>373.2</td><td>302.0</td></tr></table>

图2.12：深度Q值网络训练对照实验[1]。

# 2.6 忆阻突触阵列实现

J. Joshua Yang和Qiangfei Xia教授课题组提出了一种基于忆阻突触阵列的深度Q值网络实现方案[2]。如图2.13所示，其基本单元是1晶体管1忆阻器（1T1M），阵列规模是  $128 \times 64$  ，根据所使用的深度Q值网络大小，将其划分为3个区域，分别对应3层神经网络的突触矩阵。

在器件操作层面，该工作使用了一种新的忆阻突触权重更新方法。如图2.13(d)所示，在置态时，从位线对忆阻器的底电极端施加一个统一幅值的置态电压脉冲，而从字线对晶体管栅极施加一个正电压脉冲。该栅极电压脉冲幅值不同，对忆阻器置态时的限流效果不同，导致忆阻器的末态电导不同。这里需要特别注意，只要忆阻突触器件的长时程增强（LTP）特性足够好，就可以从器件的LTP曲线直接将需要的电导改变  $\Delta g$  转换为所需的栅极电压幅值，如图2.14所示。

在重置时，则是先从位线对忆阻器施加一个较大的重置电压，将忆阻器的电导态置到最低，然后重复前述置态过程，将其电导提升到期望值。

分析上述方案，我们可以看到两个特点：

第一，它根据  $\mathrm{Pd} / \mathrm{HfO}_2 / \mathrm{Ta}$  忆阻突触器件的长时程增强较好，而长时程减弱特性较差这一特点，充分利用了置态的连续性，规避了重置的突变性。

第二，该方案配合其它位线的电压脉冲设计，可以一次更新图2.13(d)的一行，也就是一整根位线上的全部忆阻突触器件电导，如图2.15c所示。

在神经网络层面，由于深度Q值学习的核心是两次前向和一次更新操作，上述忆阻突触阵列能够极大地提升Q值学习硬件执行效能。

![](images/2e16452a6ac54c6cfe7e6ce65ef145142f2507046e3d45295da4d94a379d8ece.jpg)

![](images/77cbaa43c30c3745aa5438cd7cea4721f545417984031ba6e04ce79d4b92d526.jpg)  
图2.13：基于1T1M单元的忆阻突触阵列及编程操操作[2]。

![](images/a59fa2c4807341a87785af8c66104902a93dd565a44a0c369ae08c4673db8897.jpg)  
图2.14：1T1M单元在栅压幅值控制下的电导更新[3]。

![](images/172da907ff03326ab73a00632f30c4dd2a55c43987278e66bb1c0f1e64b9d481.jpg)

![](images/6e18f6049b0d50198a8133447333e41d9f979a3a73add3c9a849810ba76c59fe.jpg)  
图2.15：1T1M忆阻突触阵列的前向、重置与更新操作[3]。

![](images/e4a1656f37822d05d415f05c5073dd9d3944e33c12a6f4c4979d88556acd564b.jpg)

![](images/9d0d18e0f7def80ab4ca10fc497cc864661afa13cb67fc3d6b9114b79a5c8cac.jpg)

# 3 基于脉冲的深度Q值网络

前述章节讨论的是基于模拟计算的深度Q值网络。在一些实际应用中，基于脉冲的神经网络会有显著优势。以基于动态视觉传感器（DVS）的智能驾驶为例，动态视觉传感器输送给神经网络的是一系列的地址-事件信息，适合直接使用脉冲信号编码。很显然，这种情况下使用脉冲神经网络处理更加方便。相反，假如沿用上述章节基于模拟计算的深度Q值网络来处理，那么首先需要把动态视觉传感器输入的信号翻译成一帧一帧的模拟值图像，再输入给神经网络。这种处理方式就完全失去了使用动态视觉传感器的意义。

那么如何从基于模拟计算的深度Q值网络，转换到基于脉冲的深度Q值网络呢？目前主要有两种思路，一种是仿照脉冲神经网络监督学习方法例如 SpikeProp，直接训练脉冲形式的深度Q值网络，即脉冲Q值学习(spiking Q learning)；另一种是首先训练基于模拟计算的深度Q值网络，即传统ANN形式的深度Q值网络，然后将训练好的网络权重归一化转移到脉冲形式的深度Q值网络，即策略迁移（policy transfer）。

# 3.1 脉冲Q值学习

借鉴前述章节讲述的“脉冲传播算法”，使用脉冲发放时间(spike timing)来定义状态-行动价值函数  $Q(s, a)$ ，然后采用梯度下降法极小化  $Q(s, a)$  前后迭代的差值。

该方案在执行中遇到的问题也与SpikeProp算法类似，在处理多层神经网络时，经常会收敛较慢甚至不能收敛。一种改进的方法是引入类似“”，对未激发神经元的做强制激发处理。

如图3.1所示，本书作者课题组博士生付嘉炜同学等以类脑智能驾驶应用场景“车道保持”（lane keeping）为例，以动态视觉传感器的脉冲信号为输入，构建了执行Q值学习的脉冲神经网络，并在训练中引入上述强制激发等策略，在车道保持任务中取得了较好训练效果。

神经网络训练与推断结果显示，上述方法能够有效提升脉冲Q值学习的训练收敛率以及测试通过率[4]。

![](images/02cac5b6e132880a3570ab637d461d50692d4dd0d655cb025502b0b36c39be33.jpg)  
(a)  
(c)

![](images/c19296f3d9d1d010273fff96c76a2c1345ab40bffffe49fb750b307e5406407c.jpg)  
(b)

![](images/79b341c2732249030206b12c360eb5a96f0777a84f1c4403de427e2b55d12ec3.jpg)

![](images/3686922f539a94f93d74ecedef7c81d5283efe9e1aa9ffdfdcfca51a53e552e0.jpg)  
(d)  
图3.1：基于脉冲Q值网络的车道保持。(a): 借助CoppeliaSim机器人仿真平台，搭建了车道保持任务的虚拟系统，包含赛道、安装了动态视觉传感器的小车。(b): 脉冲深度Q值网络示意图。输入直接来自动态视觉传感器，而两个输出神经元分别控制小车的左右轮速。(c): 动态视觉传感器拍摄的实时车道情况。(d): 随着强化学习训练轮数增加，车道保持效果越来越优，直到完全实现绕赛道一周。

# 3.2 策略迁移

![](images/1fbcc18348a8fa6edc9df00fa6cfdb8ff1ca25c9656e34205362588415e61e64.jpg)  
图3.2：基于脉冲的深度Q值网络：策略迁移[5]。

另一种常用策略如图3.2所示。首先采用基于模拟计算的深度Q值网络训练，训练成功后会得到一组带标签的数据  $\{(s, a)\}$  。然后，将这组带标签的存储数据乱序化，也就是随机化（想一想，为什么要这么处理？）。接下来，建立一个新的模拟神经网络，以  $\{s\}$  为输入，以  $\{a\}$  为输出，根据前述标签采用标准的监督学习算法训练，得到跟第一步深度Q值网络完全一致的结果。到这里，我们就可以说，第一步的深度Q值网络跟第二步的监督学习型模拟神经网络是等效的，因为给定相同的输入，它们会给出相同的输出。这就是所谓的策略迁移。

第三步，还需要将第二步的模拟神经网络转换成脉冲神经网络。关于这一形式转换，已经发展出若干方法，基本思想是将第二步训练成功的模拟神经网络权重归一化，再赋值给脉冲神经网络。而这一步处理的意义在于，生成的脉冲神经网络现在可以直接处理来自动态视觉传感器的输入数据，如果进一步做到神经网络输出直接对接主体的动作控制，那么就实现了所谓的“端到端”(end-to-end)计算。

慕尼黑工业大学Zhenshan Bing等人的研究工作显示，上述方法用于处理“车道保持”应用场景时，达到了与模拟值深度Q值网络同等的车道保持效果[5]。

思考题：采用策略迁移方案，第一步的模拟值深度Q值网络（DQN）与第二步的模拟值深度监督学习神经网络（ANN），它们真的是完全等价的吗？还是说只对“训练集”等价，而最重要的“泛化”结果两者完全相同吗？如果是，你能从数学上证明吗？如果不是，会导致什么后果

# 4 本章小结

# 1 强化学习的Bellman方程：

1.1 Bellman方程是描述未来总收益的，因此是一个迭代方程。求解它全局最大值的要害是如何避开当前收益极大值的陷阱。  
1.2 Bellman方程是基于马可夫过程假设的，即下一个状态只跟当前状态有关，跟更远的状态无直接关系。如果没有这个假设，则意味着过去全部时刻的各个状态均对下一个状态有贡献，只是不同的贡献有一个随时间演化的衰减因子。换句话说，Bellman方程将是一个时域卷积形式的方程，求解极其困难。

# 2 从Q值表到深度Q值网络到深度确定策略梯度：

2.1 Q值表以智能体的状态、动作为表的行、列坐标，也就是说它的状态、动作都是离散的。这意味着假如在测试中出现了一个Q值表中没有的新状态，Q值表方案将无法处理。因此，Q值表方案适合处理只具有有限个状态、有限个动作的智能体。  
2.2 深度Q值网络则是以若干个输入神经元编码状态，用不同的输出神经元代表不同的动作。我们知道输入神经元的赋值是可以连续的，这意味着它们不仅可以编码训练集中的有限个状态，还可以编码训练集中不存在的任意状态。换句话说，从Q值表到深度Q值网络，出现了状态泛化的飞跃。  
2.3 深度确定性策略梯度（deep deterministic policy gradient, DDPG）方案在此基础上更进一步，通过动作子网（action network）将智能体的动作编码也连续化了，从而实现了动作处理的泛化，因此能够处理由连续状态、连续动作构成的实际智能体系统。

# 3 脉冲编码Q值学习：

3.1 当前主要有两种方案，一种是利用深度强化学习与监督学习的数学形式相似性，仿照SpikeProp方案采用脉冲发放时间编码；另一种是策略迁移，先在模拟域训练好深度Q值网络，获得带标签的“状态-动作”数据集，然后构建深度神经网络，基于上述数据集采用监督学习方法训练它，最后通过突触权重归一化处理等方案，将在模拟域训练完成的神经网络转化为脉冲神经网络。  
3.2 脉冲编码Q值学习的优势是能够直接处理来自传感器的脉冲式输入信号。这些传感信号通常是“事件型”，即有事件发生时才会有脉冲传感信号，这就大大提升了神经网络处理信息的能效比。  
3.3 研究人员发展脉冲神经网络的本意就是希望用来处理时序问题，而不是模拟神经网络已经处理得很好的静态图像学习问题。而时序问题也分为时序信号识别与行为决策等不同种类。前者如语音、视频识别，特点是用来训练和测试的时序信号本身不会变化；后者的信号序列实际上是动态变化的，随着当前神经网络输出决策的不同而后续输入不同。显而易见，后者的挑战更大，应用价值也更高，而脉冲Q值网络有望成为处理后者的利器。

# 4忆阻突触阵列实现：

4.1 如果只看权重更新公式，深度Q值学习与模拟值神经网络的监督学习并无显著区别。而从硬件角度，考虑到前者每轮迭代需要两次前向、一次更新，基于忆阻突触阵列的硬件实现方案更充分地利用了矢量矩阵相乘实现前向运算的操作优势，因此能效比是显著优于其它方案的。  
4.2 与监督学习不同，深度Q值学习是没有固定标签或者说固定的期望值。它必须自己来猜测、尝试当前设定的目标是不是全局最优的期望值。因此，在实际执行中需要引入一系列策略，例如“探索-利用”、双网络不同步更新等。

如何充分利用忆阻器自身特性，高效率实现这些策略，是值得深入研究的方向。例如“探索-利用”，它需要对输出神经元引入随机开关，且这个随机开关的概率是随着训练次数增多而缓慢衰

减的。相比构建复杂的辅助电路模块来实现它，是否可以利用忆阻器内禀的随机性，以及这种随机开关的总体概率是受电压控制这些特性，来高效实现“探索-利用”策略呢？

# 参考文献

[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.  
[2] Zhongrui Wang, Can Li, Wenhao Song, Mingyi Rao, Daniel Belkin, Yunning Li, Peng Yan, Hao Jiang, Peng Lin, Miao Hu, John Paul Strachan, Ning Ge, Mark Barnell, Qing Wu, Andrew G. Barto, Qinru Qiu, R. Stanley Williams, Qiangfei Xia, and J. Joshua Yang. Reinforcement learning with analogue memristor arrays. Nature Electronics, 2(3):115-124, 2019.  
[3] Can Li, Daniel Belkin, Yunning Li, Peng Yan, Miao Hu, Ning Ge, Hao Jiang, Eric Montgomery, Peng Lin, Zhongrui Wang, Wenhao Song, John Paul Strachan, Mark Barnell, Qing Wu, R. Stanley Williams, J. Joshua Yang, and Qiangfei Xia. Efficient and self-adaptive in-situ learning in multilayer memristor neural networks. Nature Communications, 9(1):2385, 2018.  
[4] 张大友. 脉冲神经网络监督学习研究. 博士学位论文, 华中科技大学, May 2023.  
[5] Zhenshan Bing, Claus Meschede, Guang Chen, Alois Knoll, and Kai Huang. Indirect and direct training of spiking neural networks for end-to-end control of a lane-keeping vehicle. _Neural Networks_, 121:21 - 36, 2020.